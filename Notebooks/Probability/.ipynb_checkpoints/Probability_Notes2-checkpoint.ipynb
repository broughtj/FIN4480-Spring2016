{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elementary Probability Review Continued\n",
    "\n",
    "This is a review of elementary probability that will be useful for our\n",
    "study of asset pricing. It is based on coverage in Greene, as well as\n",
    "Wooldridge.\n",
    "\n",
    "The **cumulative distribution function** (**CDF**) of the random\n",
    "variable $X$ is:\n",
    "\n",
    "$$F(X) = P(X \\leq x)$$\n",
    "\n",
    "For discrete random variables it is obtained by summing the PDF over all\n",
    "values $x_{j}$ such that $x_{j} \\leq x$.\n",
    "\n",
    "For a continuous random variable, $F(X)$ is the area under the PDF,\n",
    "$f(x)$ to the left of $x$.\n",
    "\n",
    "Because it is a probability, $0 \\leq F(X) \\leq 1$.\n",
    "\n",
    "If $x_{1} < x_{2}$ then $P(X \\leq x_{1}) \\leq P(X \\leq x_{2})$, that is\n",
    "$F(x_{1}) \\leq F(x_{2})$.\n",
    "\n",
    "Two important properties of CDFs that are useful for computing\n",
    "probabilities are the following:\n",
    "\n",
    "-   For and number $c$, $P(X > c) = 1 - F(c)$\n",
    "\n",
    "-   For any numbers $a$ and $b$, $P(a \\leq X \\leq b) = F(b) - F(a)$\n",
    "\n",
    "For continuous random variables the inequalities in probability\n",
    "statements are not strict:\n",
    "\n",
    "$$P(X \\geq c) = P( > c)$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(a < X < b) &= P(a \\leq X \\leq b) \\\\\n",
    "             &= P(a \\leq X < b)    \\\\\n",
    "             &= P(a < X \\leq b)\\end{aligned}$$\n",
    "\n",
    "Let $X$ and $Y$ be discrete random variables. Then for $(X,Y)$ a **joint\n",
    "distribution** which is fully described by the **joint probability\n",
    "density function** of $(X,Y)$:\n",
    "\n",
    "$$f_{XY}(x, y) = P(X = x, Y = y)$$\n",
    "\n",
    "$X$ and $Y$ are said to be independent if, and only if:\n",
    "\n",
    "$$f_{XY}(x,y) = f_{X}(x) f_{Y}(y) \\quad \\mbox{for every $x$ and $y$}$$\n",
    "\n",
    "where $f_{X}$ is the PDF of the random variable $X$, and $f_{Y}$ is the\n",
    "PDF of random variable $Y$.\n",
    "\n",
    "$f_{X}$ and $f_{Y}$ are referred to as the **marginal probability\n",
    "density functions**.\n",
    "\n",
    "The discrete case is the easiest to grok. If $X$ and $Y$ are discrete\n",
    "and independent then\n",
    "\n",
    "$$P(X=x, Y=y) = P(X=x)P(Y=y)$$\n",
    "\n",
    "Note: If $X$ and $Y$ are independent then finding the joint PDF only\n",
    "requires knowledge of $P(X=x)$ and $P(Y=y)$\n",
    "\n",
    "Example: Consider a basketball player shooting two free throws. Let $X$\n",
    "be the Bernoulli random variable equal to $1$ if he makes the first free\n",
    "throw, and $0$ otherwise. Let $Y$ be the Bernoulli random variable equal\n",
    "to $1$ if he makes the second free throw. Suppose that he is an $80\\%$\n",
    "free throw shooter, so that $P(X=1) = P(Y=1) = 0.80$. What is the\n",
    "probability of making both free throws?\n",
    "\n",
    "If $X$ and $Y$ are independent:\n",
    "$P(X=1, Y=1) = P(X=1)P(Y=1) = (0.8) ( 0.8) = 0.64$. Thus, a $64\\%$\n",
    "chance of making both.\n",
    "\n",
    "Independence is often reasonable in more complicated situations. In the\n",
    "airline example, suppose that $n$ is the number of reservations booked.\n",
    "For each $i = 1, 2, \\ldots, n$ let $Y_{i}$ denote the Bernoulli random\n",
    "variable indicating whether or not customer $i$ shows up for the flight.\n",
    "\n",
    "Let $\\theta$ again denote the probability of success (showing up for the\n",
    "reservation). Each $Y_{i} \\sim \\mbox{Bernoulli($$)}$.\n",
    "\n",
    "The variable of primary interest is the total number of customers\n",
    "showing up out of the $n$ reservations: call this $X$.\n",
    "\n",
    "$$X = Y_{1} + Y_{2} + \\ldots + Y_{n}$$\n",
    "\n",
    "Assume that $P(Y_{i} = 1) = \\theta$ for every $Y_{i}$, and further that\n",
    "they $Y_{i}$ are independent. Then $X$ has a **binomial distribution**,\n",
    "which we write in shorthand as: $X \\sim \\mbox{Binomial($n$, $$)}$. The\n",
    "binomial PDF is the following:\n",
    "\n",
    "$$f(x) = {n \\choose x} \\theta^{x} (1-\\theta)^{n-x} \\quad \\mbox{for $x = 0, 1, 2, \\ldots, n$}$$\n",
    "\n",
    "Note: ${n \\choose x} = \\frac{n!}{x! (n-x)!}$, and is read as “n choose\n",
    "x”.\n",
    "\n",
    "Example: If the flight has $100$ seats and $n = 120$ and $\\theta = 0.85$\n",
    "then:\n",
    "\n",
    "$$P(X > 100) = P(X=101) + P(X=102) + \\ldots + P(X=120)$$\n",
    "\n",
    "In econometrics we are usually interested in how one variable $Y$ is\n",
    "related to one or more other variables. For now, consider only one such\n",
    "variable $X$. What we can know about how $X$ affects $Y$ is contained in\n",
    "the **conditional distribution** of $Y$ given $X$. This information is\n",
    "summarized in the **conditional probability distribution function**:\n",
    "\n",
    "$$f_{Y|X}(y|x) = \\frac{f_{XY}(x, y)}{f_{X}(x)}$$\n",
    "\n",
    "In the discrete case: $f_{Y|X}(y|x) = P(Y=y|X=x)$, which we read as the\n",
    "probability that $Y=y$ given that $X=x$.\n",
    "\n",
    "If $X$ and $Y$ are independent, then the knowledge of $X$ tells us\n",
    "nothing about $Y$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "f_{Y|X}(y|x) &= f_{Y}(y) \\quad \\mbox{and} \\\\\n",
    "f_{X|Y}(x|y) &= f_{X}(x)\\end{aligned}$$\n",
    "\n",
    "Example: Free throw shooting again. Assume the conditional PDF is given\n",
    "by the following:\n",
    "\n",
    "-   $f_{Y|X}(1|1) = 0.85$, and $f_{Y|X}(0|1) = 0.15$.\n",
    "\n",
    "-   $f_{Y|X}(1|0) = 0.70$, and $f_{Y|X}(0|0) = 0.30$.\n",
    "\n",
    "These are not independent. The probability of making the second free\n",
    "throw depends on whether or not the first free throw was made. We can\n",
    "calculate $P(X=1, Y=1)$ if we know $P(X=1)$. Assume the probability of\n",
    "making the first free throw is $P(X=1) = 0.80$. Then:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(X=1, Y=1) &= P(Y=1|X=1) \\times P(X=1) \\\\\n",
    "            &= (0.85) \\times (0.80)     \\\\\n",
    "            &= 0.68\\end{aligned}$$\n",
    "\n",
    "The **expected value** is a measure of central tendency. It is one of\n",
    "the most important probabilistic concepts in econometrics. If $X$ is a\n",
    "random variable the **expected value** (or expectation) of $X$, denoted\n",
    "$E(X)$ and sometimes $\\mu$, is a weighted average of all possible values\n",
    "of $X$. The weights are determined by the PDF.\n",
    "\n",
    "Consider the case of a discrete random variable. Let $f(x)$ denote the\n",
    "PDF of $X$. The expected value of $X$ is the weighted average:\n",
    "\n",
    "$$E(X) = x_{1}f(x_{1}) + x_{2}f(x_{2}) + \\ldots + x_{k}f(x_{k}) = \\sum\\limits_{j=1}^{k} x_{j}f(x_{j})$$\n",
    "\n",
    "Example: Suppose $X$ takes on the values $-1$, $0$, and $2$ with\n",
    "probabilities $\\frac{1}{8}$, $\\frac{1}{2}$, $\\frac{3}{8}$. Then\n",
    "\n",
    "$$E(X) = (-1)(\\frac{1}{8}) + (0)(\\frac{1}{2}) + (2)(\\frac{3}{8}) = \\frac{5}{8}$$\n",
    "\n",
    "Note: $E(X)$ can take on values that are not even possible outcomes of\n",
    "$X$.\n",
    "\n",
    "If $X$ is a continuous random variable then\n",
    "\n",
    "$$E(X) = \\int\\limits_{-\\infty}^{\\infty} xf(x)dx$$\n",
    "\n",
    "This is still interpreted as a weighted average.\n",
    "\n",
    "Given a random variable $X$ and a function $g(\\cdot)$, we can create a\n",
    "new random variable $g(X)$. For example, if $X$ is a random variable,\n",
    "then so is $X^{2}$ or $log(X)$ (for $x > 0$).\n",
    "\n",
    "The expected value of $g(X)$ is given by\n",
    "\n",
    "$$E[g(X)] = \\sum\\limits_{j=1}^{k} g(x_{j}) f_{X}(x_{j})$$\n",
    "\n",
    "or\n",
    "\n",
    "$$E[g(X)] = \\int\\limits_{-\\infty}^{\\infty} g(x)f_{X}(x)dx$$\n",
    "\n",
    "Example: For the random variable above let $g(X) = X^{2}$. Then\n",
    "\n",
    "$$E(X^{2}) = (-1)^{2}(\\frac{1}{8}) + (0)^{2}(\\frac{1}{2}) + (2)^{2}(\\frac{3}{8}) = \\frac{13}{8}$$\n",
    "\n",
    "Note: $E[g(X)] \\neq g[E(X)]$.\n",
    "\n",
    "Properties of Expected Values:\n",
    "\n",
    "-   **Property E1:** For any constant $c$, $E(c) = c$.\n",
    "\n",
    "-   **Property E2:** For any constants $a$ and $b$,\n",
    "    $E(aX + b) = aE(X) + b$.\n",
    "\n",
    "-   **Property E3:** If ${a_{1}, a_{2}, \\ldots, a_{n}}$ are constants\n",
    "    and ${X_{1}, X_{2}, \\ldots, X_{n}}$ are random variables then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "-   $E(a_{1}X_{1} + a_{2}X_{x} + \\ldots + a_{n}X_{n}) = a_{1}E(X_{1}) + a_{2}E(X_{2}) + \\ldots + a_{n}E(X_{n})$\n",
    "\n",
    "-   Or\n",
    "    $E(\\sum\\limits_{i=1}^{n} a_{i}X_{i}) = \\sum\\limits_{i=1}^{n} a_{i}E(X_{i})$\n",
    "\n",
    "-   A special case is when each $a_{i} = 1$ so that\n",
    "    $E(\\sum\\limits_{i=1}^{n} E(X_{i})) = \\sum\\limits_{i=1}^{n} E(X_{i})$,\n",
    "    or in other words the expected value of a sum, is the sum of the\n",
    "    expected values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Expected revenue at a pizzeria. $X_{1}$, $X_{2}$, and $X_{3}$\n",
    "are the number of small, medium, and large pizzas sold during the day.\n",
    "Suppose $E(X_{1}) = 25$, $E(X_{2}) = 57$, and $E(X_{3}) = 40$. Prices\n",
    "are $\\$5.50$ for a small, $\\$7.60$ for a medium, and $\\$9.15$ for a\n",
    "large. Then expected revenue is the following\n",
    "\n",
    "$$\\begin{aligned}\n",
    "E(5.50X_{1} + 7.60X_{2} + 9.15X_{3}) &= 5.50E(X_{1}) + 7.60E(X_{2}) + 9.15E(X_{3}) \\\\\n",
    "                                     &= 5.50(25) + 7.60(57) + 9.15(40) \\\\\n",
    "                                     &= 936.70 \\end{aligned}$$\n",
    "\n",
    "The outcome on any given day will differ from this, but this is the\n",
    "expected revenue.\n",
    "\n",
    "If $X \\sim \\mbox{Binomial($n$, $$)}$ then $E(X) = n\\theta$. The expected\n",
    "number of successes in $n$ Bernoulli trials is $n\\theta$. We can see\n",
    "this by writing\n",
    "\n",
    "$$X = Y_{1} + Y_{2} + \\ldots + Y_{n} \\quad \\mbox{where each $Y_{i} \\sim$ Bernoulli($\\theta$)}$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\\begin{aligned}\n",
    "E(X) &= \\sum\\limits_{i=1}^{n} E(Y_{i}) \\\\\n",
    "     &= \\sum\\limits_{i=1}^{n} \\theta   \\\\\n",
    "     &= n\\theta\\end{aligned}$$\n",
    "\n",
    "Example: Consider the airline problem with $n=120$ and $\\theta = 0.85$.\n",
    "Then $E(X) = n\\theta = 120(0.85) = 102$, which is too many.\n",
    "\n",
    "The **median** is another measure of central tendency. If $X$ is\n",
    "continuous then the median is the value $m$ such that one–half of the\n",
    "area under the PDF is to the left of $m$, and one–half is to the right\n",
    "of $m$.\n",
    "\n",
    "If $X$ is discrete and takes on an odd number of finite values, the\n",
    "median is obtained by ordering the possible outcomes of $X$ and\n",
    "selecting the middle value.\n",
    "\n",
    "Example: For the sample $\\{-4, 0, 2, 8, 10, 13, 17\\}$ the median is $8$.\n",
    "\n",
    "If $X$ takes on an even number of values, then the median is the average\n",
    "of the two middle values.\n",
    "\n",
    "Example: For the sample $\\{-5, 3, 9, 17\\}$ the median is\n",
    "$\\frac{3+9}{2} = 6$.\n",
    "\n",
    "For a random variable let $E(X) = \\mu$. There are various ways to\n",
    "measure how far $X$ is from its expected value. One of the simplest is\n",
    "the squared distance:\n",
    "\n",
    "$$(X - \\mu)^{2}$$\n",
    "\n",
    "This eliminates the sign, which corresponds with our intuitive notion of\n",
    "a distance measure. It treats values above and below $\\mu$\n",
    "symmetrically.\n",
    "\n",
    "The **variance** is defined as follows:\n",
    "\n",
    "$$Var(X) = E[(X - \\mu)^{2}]$$\n",
    "\n",
    "The variance is sometimes denoted by $\\sigma_{X}^{2}$ or just\n",
    "$\\sigma^{2}$ when the random variable is understood to be $X$.\n",
    "\n",
    "Note:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\sigma^{2} &= E(X^{2} - 2X\\mu + \\mu^{2}) \\\\\n",
    "            &= E(X^{2}) - 2\\mu^{2} + \\mu^{2} \\\\\n",
    "            &= E(X^{2}) - \\mu^{2}\\end{aligned}$$\n",
    "\n",
    "Example: If $X \\sim$ Bernoulli($\\theta$) we know that $E(X) = \\theta$.\n",
    "Since $X^{2} = X$ it follows that $E(X^{2}) = \\theta$. Then\n",
    "$Var(X) = E(X^{2}) - \\mu^{2} = \\theta - \\theta^{2} = \\theta(1 - \\theta)$.\n",
    "\n",
    "Properties of variance:\n",
    "\n",
    "-   **Property VAR1:** $Var(X) = 0$ if, and only if for every $c$ such\n",
    "    that $P(X=c) = 1$, in which case $E(X) = c$.\n",
    "\n",
    "-   **Property VAR2:** For constants $a$ and $b$\n",
    "    $Var(aX + b) = a^{2} Var(X)$.\n",
    "\n",
    "The **standard deviation** is related to the variance as follows:\n",
    "$sd(X) = \\sqrt{Var(x)}$. The standard deviation is often denoted\n",
    "$\\sigma_{x}$ or just $\\sigma$.\n",
    "\n",
    "Properties of the standard deviation:\n",
    "\n",
    "-   **Property SD1:** For a constant $c$, $sd(c) = 0$.\n",
    "\n",
    "-   **Property SD2:** For constants $a$ and $b$ $sd(aX + b) = |a|sd(X)$.\n",
    "\n",
    "Given a random variable $X$, we can define a new random variable $Z$ by\n",
    "\n",
    "$$Z = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "or $Z = aX + b$ where $a = \\frac{1}{\\sigma}$ and\n",
    "$b = \\frac{-\\mu}{\\sigma}$. Then\n",
    "$E(Z) = aE(X) + b = \\frac{\\mu}{\\sigma} - \\frac{\\mu}{\\sigma} = 0$.\n",
    "\n",
    "The variance is\n",
    "$Var(Z) = a^{2}Var(X) = \\frac{\\sigma^{2}}{\\sigma^{2}} = 1$. Thus the new\n",
    "random variable has $\\mu = 0$ and $\\sigma^{2} = 1$. This is known as\n",
    "**standardizing** a random variable.\n",
    "\n",
    "Example: Suppose $E(X) = 2$ and $Var(X) = 9$ then $Z = \\frac{X - 2}{3}$.\n",
    "\n",
    "While the joint distribution completely describes the relationship\n",
    "between two random variables it is often useful to have a summary\n",
    "measure of how, on average, two random variables vary with one another.\n",
    "\n",
    "The **covariance** is defined as follows:\n",
    "\n",
    "$$Cov(X,Y) = E[(X-\\mu_{X})(Y - \\mu_{Y})]$$\n",
    "\n",
    "The covariance is often denoted by $\\sigma_{XY}$. If $\\sigma{XY} > 0$\n",
    "then on average when $X$ is above its mean $Y$ is also above its mean.\n",
    "If $\\sigma_{XY} < 0$ then on average when $X$ is above its mean $Y$ is\n",
    "below its mean, and vice versa.\n",
    "\n",
    "Note:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "Cov(X,Y) &= E[(X - \\mu_{X})(Y - \\mu_{Y})] \\\\\n",
    "         &= E[(X - \\mu_{X}) Y]            \\\\\n",
    "         &= E(XY) - \\mu_{X}\\mu_{Y}\\end{aligned}$$\n",
    "\n",
    "Properties of covariance:\n",
    "\n",
    "-   **Property COV1:** If $X$ and $Y$ are independent then\n",
    "    $Cov(X,Y) = 0$. Note: the converse is not true. Zero $Cov(X, Y)$\n",
    "    does not imply independence.\n",
    "\n",
    "-   **Property COV2:** For any constants $a_{1}$, $b_{2}$, $a_{2}$, and\n",
    "    $b_{2}$ $Cov(a_{1}X + b_{1}, a_{2}Y + b_{2}) = a_{1}a_{2}Cov(X,Y)$.\n",
    "\n",
    "-   **Property COV3:** $|Cov(X,Y)| \\leq sd(X)sd(Y)$.\n",
    "\n",
    "Note: property COV2 suggests that $Cov(X,Y)$ depends upon how the random\n",
    "variables are measured, not only on how strongly they are related. In\n",
    "other words, scale matters for $Cov(X,Y)$.\n",
    "\n",
    "The **correlation coefficient** is defined as\n",
    "\n",
    "$$Corr(X,Y) = \\frac{Cov(X,Y)}{sd(X)sd(Y} = \\frac{\\sigma_{XY}}{\\sigma_{X}\\sigma_{Y}}$$\n",
    "\n",
    "The correlation coefficient is sometimes denoted by $\\rho_{XY}$.\n",
    "\n",
    "Properties of correlation:\n",
    "\n",
    "-   **Property CORR1:** $-1 \\leq Corr(X,Y) \\leq 1$.\n",
    "\n",
    "-   **Property CORR2:** For constants $a_{1}$, $b_{1}$, $a_{2}$, and\n",
    "    $b_{2}$ with $a_{1}a_{2} > 0$\n",
    "    $Corr(a_{1}X + b_{1}, a_{2}Y + b_{2}) = Corr(X,Y)$. If\n",
    "    $a_{1}a_{2} < 0$ then\n",
    "    $Corr(a_{1}X + b_{1}, a_{2}Y + b_{2}) = -Corr(X,Y)$.\n",
    "\n",
    "With covariance and correlation defined we state further properties of\n",
    "the variance:\n",
    "\n",
    "-   **Property VAR3:** For constants $a$ and $b$,\n",
    "    $Var(aX + bY) = a^{2} Var(X) + b^{2} Var(Y) + 2abCov(X,Y)$.\n",
    "\n",
    "-   **Property VAR4:** If $\\{X_{1}, X_{2}, \\ldots, X_{n}\\}$ are pairwise\n",
    "    uncorrelated and $\\{a_{i}: i=1, \\ldots, n \\}$ are constants then\n",
    "    $Var(\\sum\\limits_{i=1}^{n} a_{i} X_{i}) = \\sum\\limits_{i=1}^{n} a_{i}^{2} Var(X_{i})$.\n",
    "\n",
    "The **conditional mean** is defined as follows:\n",
    "\n",
    "$$E(Y|x) = \\sum\\limits_{j=1}^{m} y_{j} f_{Y|X}(y_{j}|x)$$\n",
    "\n",
    "Example: Let $(X,Y)$ represent the population of all working\n",
    "individuals, where $X$ is years of education and $Y$ is hourly wages.\n",
    "Then $E(Y|X=12)$ is the average hourly wage for all the people in the\n",
    "population with $12$ years of education (roughly high school education).\n",
    "$E(Y|X=16)$ is the average hourly wage for all people with $16$ years of\n",
    "education.\n",
    "\n",
    "A typical situation in econometrics will look like the following:\n",
    "\n",
    "$$E(WAGE|EDUC) = 1.05 + 0.45 EDUC$$\n",
    "\n",
    "If this linear relationship holds then for $8$ years of education the\n",
    "expected hourly wage is $1.05 + 0.45(8) = 4.65$ of $\\$4.65$ per hour.\n",
    "\n",
    "Properties of conditional expectations:\n",
    "\n",
    "-   **Property CE1:** $E[c(X)|X] = c(X)$ for any function $c(X)$. In\n",
    "    other words, functions act as constants. For example,\n",
    "    $E[X^{2}|X] = X^{2}$. If we know $X$ we also know $X^{2}$.\n",
    "\n",
    "-   **Property CE2:** For funtions $a(X)$ and $b(X)$,\n",
    "    $E[a(X)Y + b(X)|X] = a(X)E(Y|X) + b(X)$. For example, consider the\n",
    "    random variable $XY + 2X^{2}$.\n",
    "    $E(XY + 2X^{2}|X) = XE(Y|X) + 2X^{2}$.\n",
    "\n",
    "-   **Property CE3:** If $X$ and $Y$ are independent then\n",
    "    $E(Y|X) = E(Y)$.\n",
    "\n",
    "-   **Property CE4:** $E[E(Y|X)] = E(Y)$. This is known as the Law of\n",
    "    Iterated Expectations.\n",
    "\n",
    "-   **Property CE5:** $E(Y|X) = E[E(Y|X,Z)|X]$.\n",
    "\n",
    "-   **Property CE6:** If $E(Y|X) = E(Y)$ then $Cov(X,Y) = 0$ and\n",
    "    $Corr(X,Y) = 0$.\n",
    "\n",
    "The **conditional variance** is defined as follows:\n",
    "\n",
    "$$Var(Y|X=x) = E(Y^{2}|X) - [E(Y|X)]^{2}$$\n",
    "\n",
    "Properties of conditional variance:\n",
    "\n",
    "-   **Property CV1:** If $X$ and $Y$ are independent then\n",
    "    $Var(Y|X) = Var(Y)$.\n",
    "\n",
    "The **normal probability density function** is defined as follows:\n",
    "\n",
    "$$f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp{\\frac{-(X-\\mu)^{2}}{2\\sigma^{2}}}, \\quad \\mbox{for $-\\infty < x < \\infty$}$$\n",
    "\n",
    "where $E(X) = \\mu$ and $Var(X) = \\sigma^{2}$. When is a random variable\n",
    "is normally distributed we write $X \\sim N(\\mu, \\sigma^{2})$.\n",
    "\n",
    "A special case is the **standard normal distribution**, which is defined\n",
    "as follows:\n",
    "\n",
    "$$\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp{\\frac{-z^{2}}{2}}, \\quad \\mbox{for $-\\infty < z < \\infty$}$$\n",
    "\n",
    "The standard normal cumulative distribution function is denoted by\n",
    "$\\Phi(z) = P(Z \\leq z)$. Using some basic facts from probability we\n",
    "arrive at the following helpful formulas:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(Z > z)  &= 1 - \\Phi(z) \\\\\n",
    "P(Z < -z) &= P(Z > z) \\\\\n",
    "P(a \\leq Z \\leq b) &= \\Phi(b) - \\Phi(a)\\end{aligned}$$\n",
    "\n",
    "Properties of the normal distribution:\n",
    "\n",
    "-   **Property NORMAL1:** If $X \\sim N(\\mu, \\sigma^{2})$ then\n",
    "    $\\frac{(X - \\mu)}{\\sigma} \\sim N(0, 1)$.\n",
    "\n",
    "-   **Property NORMAL2:** If $X \\sim N(\\mu, \\sigma^{2})$ then\n",
    "    $aX + b \\sim N(a\\mu + b, a^{2}\\sigma^{2})$.\n",
    "\n",
    "-   **Property NORMAL3:** If $X$ and $Y$ are jointly normally\n",
    "    distributed, then they are independent if, and only if\n",
    "    $Cov(X,Y) = 0$.\n",
    "\n",
    "-   **Property NORMAL4:** Any linear combination of independent,\n",
    "    identically distributed normal random variables has a normal\n",
    "    distribution.\n",
    "\n",
    "Example: Let $X_{i}$ for $i = 1, 2, \\mbox{and}, 3$, be independent\n",
    "random variables distributed as $N(\\mu, \\sigma^{2})$. Define\n",
    "$W = X_{1} + 2X_{2} - 3X_{3}$. Then $W$ is normally distributed. We can\n",
    "solve for the mean and variance as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "E(W) &= E(X_{1}) + 2E(X_{2}) - 3E(X_{3}) = \\mu + 2\\mu - 3\\mu = 0 \\\\\n",
    "Var(W) &= Var(X_{1}) + 4Var(X_{2}) + 9Var(X_{3}) = 16\\sigma^{2}\\end{aligned}$$\n",
    "\n",
    "The **chi–square distribution** is obtained directly from independent,\n",
    "standard normal random variables. Let $Z_{i}$, $i = 1, 2, \\ldots, n$, be\n",
    "independent random variables, each distributed as standard normal.\n",
    "Define a new random variable as the sum of the squares of the individual\n",
    "$Z_{i}$:\n",
    "\n",
    "$$X = \\sum\\limits_{i=1}^{n} Z_{i}^{2}$$\n",
    "\n",
    "The new random variable $X$ has a **chi–square distribution** with $n$\n",
    "**degrees of freedom**. This is often written as $X \\sim \\chi_{n}^{2}$.\n",
    "\n",
    "The **$t$ distribution** is a workhorse in classical statistics and\n",
    "econometrics. A $t$ distribution is obtained from a standard normal and\n",
    "a chi–square random variable. Let $Z$ have a standard normal\n",
    "distribution and let $X$ have a chi-square distribution with $n$ degrees\n",
    "of freedom. Also assume that $Z$ and $X$ are independent. Then the\n",
    "following random variable\n",
    "\n",
    "$$T = \\frac{Z}{\\sqrt{Z/n}}$$\n",
    "\n",
    "has a $t$ distribution with $n$ degrees of freedom. This is denoted by\n",
    "$T \\sim t_{n}$. The $t$ distribution gets its degrees of freedom from\n",
    "the chi–square random variable.\n",
    "\n",
    "Another important distribution for statistics and econometrics is the\n",
    "**$F$ distribution**. To define an $F$ random variable, let\n",
    "$X_{1} \\sim \\chi_{k_{1}}^{2}$ and $X_{2} \\sim \\chi_{k_{2}}^{2}$ and\n",
    "assume that $X_{1}$ and $X_{2}$ are independent. Then, the random\n",
    "variable\n",
    "\n",
    "$$F = \\frac{X_{1}/k_{1}}{X_{2}/k_{2}}$$\n",
    "\n",
    "has an $F$ distribution with $(k_{1}, k_{2})$ degrees of freedom. We\n",
    "denote this as $F \\sim F_{k_{1}, k_{2}}$. The order of the degrees of\n",
    "freedom is important. $k_{1}$ is the *numerator degrees of freedom* and\n",
    "$k_{2}$ is the *denominator degrees of freedom*.\n",
    "\n",
    "# References\n",
    "\n",
    "[Wooldridge, Jeffrey (2009) *Introductory Econometrics: A Modern Approach 4th Edition.*](<http://goo.gl/SIqGID>)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
