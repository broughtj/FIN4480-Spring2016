<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Tyler J. Brough" />
  <title>Mathematical Statistics Review</title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<div id="header">
<h1 class="title">Mathematical Statistics Review</h1>
<h2 class="author">Tyler J. Brough</h2>
</div>
<h1 id="introduction">Introduction</h1>
<p>This is a review of fundamental mathematical statistics that will be essential for learning econometrics. The coverage is based on Wooldridge (Appendix C).</p>
<h1 id="what-is-statistics">What is Statistics?</h1>
<p><strong>Statistical inference</strong> is the process of learning something about a population given a sample from that population. Using the tools of statistics we will seek to <em>infer</em> something about the population, given only a sample.</p>
<p>A <strong>population</strong> is a well defined group of subjects, such as individuals, firms, cities, etc.</p>
<p>By learning, we mainly mean two things:</p>
<ul>
<li>Estimation</li>
<li>Hypothesis Testing</li>
</ul>
<h1 id="example-1">Example 1</h1>
<p>An example of a population is all working adults in the US. Labor economists are interested in learning about the return to education, measured by the average increase in earnings given another year of education. It is impractical or impossible to gather data on the entire population, but she can obtain data on a subset of the population. Using the data collected a labor economist may report that her best estimate of the return to another year of education is <span class="math">7.5%</span>. This is an example of a <strong>point estimate</strong>. Or she may report a range, such as “the return to education is between <span class="math">5.6%</span> and <span class="math">9.4%</span>.” This is an example of an <strong>interval estimate</strong>.</p>
<h1 id="example-2">Example 2</h1>
<p>An urban economist might want to know whether neighborhood crime watch programs are associated with lower crime rates. After comparing crime rates of neighborhoods with and without such programs in a sample from the population, he can draw one of two conclusions: neighborhood watch programs do affect crime, or they do not. This is an example of <strong>hypothesis testing</strong>.</p>
<h1 id="population-models-parameters">Population, Models, &amp; Parameters</h1>
<p>The first step in statistical inference is to identify the population of interest. Once a population has been identified, a model for the population relationship of interest may be specified. Models involve probability distributions or features of probability distributions, and these depend on unknown parameters. <strong>Parameters</strong> are constants that determine the directions and strengths of relationships among variables.</p>
<p>In the labor economics example, the parameter of interest is the return to education in the population.</p>
<h1 id="sampling">Sampling</h1>
<p>Let <span class="math"><em>Y</em></span> be a random variable representing a population with PDF <span class="math"><em>f</em>(<em>y</em>; <em>θ</em>)</span>, which depends on a single parameter <span class="math"><em>θ</em></span>. The PDF is assumed to be known, except for <span class="math"><em>θ</em></span>. Different values of <span class="math"><em>θ</em></span> imply different population distributions, and therefore we are interested in <span class="math"><em>θ</em></span>. If we can obtain samples from the population we can learn something about <span class="math"><em>θ</em></span>.</p>
<h1 id="random-sampling">Random sampling</h1>
<ul>
<li>If <span class="math"><em>Y</em><sub>1</sub>, <em>Y</em><sub>2</sub>, …<em>Y</em><sub><em>n</em></sub></span> are independent random variables with a common PDF <span class="math"><em>f</em>(<em>y</em>; <em>θ</em>)</span> then <span class="math">{<em>Y</em><sub>1</sub>, <em>Y</em><sub>2</sub>, …, <em>Y</em><sub><em>n</em></sub>}</span> is said to be a <em>random sample</em> from <span class="math"><em>f</em>(<em>y</em>, <em>θ</em>)</span> (a random sample represented by <span class="math"><em>f</em>(<em>y</em>; <em>θ</em>)</span>).</li>
<li>When <span class="math">{<em>Y</em><sub>1</sub>, <em>Y</em><sub>2</sub>, …, <em>Y</em><sub><em>n</em></sub>}</span> is a random sample from <span class="math"><em>f</em>(<em>y</em>, <em>θ</em>)</span>, we also say that the <span class="math"><em>Y</em><sub><em>i</em></sub></span> are <strong>independent and identically distributed</strong> (or iid) random variables from <span class="math"><em>f</em>(<em>y</em>; <em>θ</em>)</span>.</li>
</ul>
<p>If family income is obtained for <span class="math"><em>n</em> = 100</span> families in the US, the incomes we observe will differ for each sample of <span class="math">100</span> that we choose. Once a sample is obtained we have a set of number <span class="math">{<em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, …, <em>y</em><sub>3</sub>}</span>, which constitute the data that we work with.</p>
<h1 id="random-sample-from-a-bernoulli-distribution">Random Sample from a Bernoulli Distribution</h1>
<p>Random samples from Bernoulli distributions are often used to illustrate statistical concepts. If <span class="math"><em>Y</em><sub>1</sub>, <em>Y</em><sub>2</sub>, …, <em>Y</em><sub><em>n</em></sub></span> are iid Bernoulli(<span class="math"><em>θ</em></span>), such that <span class="math"><em>P</em>(<em>Y</em><sub><em>i</em></sub> = 1) = <em>θ</em></span> and <span class="math"><em>P</em>(<em>Y</em><sub><em>i</em></sub> = 0) = 1 − <em>θ</em></span> then <span class="math">{<em>Y</em><sub>1</sub>, <em>Y</em><sub>2</sub>, …, <em>Y</em><sub><em>n</em></sub>}</span> constitute a random sample from a Bernoull(<span class="math"><em>θ</em></span>) distribution.</p>
<h1 id="the-airline-example">The Airline Example</h1>
<p>Consider the airline example: Each <span class="math"><em>Y</em><sub><em>i</em></sub></span> denotes whether or not passenger <span class="math"><em>i</em></span> shows up. <span class="math"><em>θ</em></span> is the probability that a randomly drawn individual from the population shows up.</p>
<h1 id="random-samples-from-the-normal-distribution">Random Samples from the Normal Distribution</h1>
<p>For many applications, random samples can be assumed to be drawn from a normal distribution. If <span class="math">{<em>Y</em><sub>1</sub>, <em>Y</em><sub>2</sub>, …, <em>Y</em><sub><em>n</em></sub>}</span> is a random sample from the Normal(<span class="math"><em>μ</em></span>, <span class="math"><em>σ</em><sup>2</sup></span>) population, the population is characterized by two parameters, the mean <span class="math"><em>m</em><em>u</em></span> and the variance <span class="math"><em>σ</em><sup>2</sup></span>.</p>
<h1 id="finite-sample-properties">Finite Sample Properties</h1>
<p><strong>Finite sample properties</strong> are properties that hold for a sample of any size, no matter how small or large (sometimes called “small sample properties” to distinguish from “asymptotic properties”).</p>
<h1 id="estimation-and-estimators">Estimation and Estimators</h1>
<p>Given a random sample drawn from a population distribution that depends on an unknown parameter <span class="math"><em>θ</em></span>. An <strong>estimator</strong> of <span class="math"><em>θ</em></span> is a rule that assigns each possible outcome of the sample a value of <span class="math"><em>θ</em></span>. The rule is specified before any sampling is carried out (regardless of the data collected).</p>
<p>Let <span class="math">{<em>Y</em><sub>1</sub>, <em>Y</em><sub>2</sub>, …, <em>Y</em><sub><em>n</em></sub>}</span> be a random sample from a population with mean <span class="math"><em>μ</em></span>. A natural estimator of <span class="math"><em>μ</em></span> is the average of the random sample:</p>
<p><br /><span class="math">$$\bar{Y} = \frac{1}{n}\sum\limits_{i=1}^{n} Y_{i}$$</span><br /></p>
<p><span class="math">$\bar{Y}$</span> is called the <strong>sample average</strong>; unlike earlier when we defined the average as a descriptive statistics, <span class="math">$\bar{Y}$</span> is now viewed as an estimator. Given any outcome of the random variables <span class="math"><em>Y</em><sub>1</sub>, <em>Y</em><sub>2</sub>, …, <em>Y</em><sub><em>n</em></sub></span>, we use the same rule to estimate <span class="math"><em>μ</em></span>: we average them. For actual outcomes <span class="math">{<em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, …, <em>y</em><sub><em>n</em></sub>}</span>, the estimate is just the average in the sample.</p>
<p><br /><span class="math">$$\bar{y} = \frac{(y_{1} + y_{2} + \cdots + y_{n})}{n}$$</span><br /></p>
<p>More generally an estimator <span class="math"><em>W</em></span> of a parameter <span class="math"><em>θ</em></span> can be expressed as:</p>
<p><br /><span class="math"><em>W</em> = <em>h</em>(<em>Y</em><sub>1</sub>, <em>Y</em><sub>2</sub>, …, <em>Y</em><sub><em>n</em></sub>)</span><br /></p>
<p>for some known function <span class="math"><em>h</em></span> of the random variables <span class="math"><em>Y</em><sub>1</sub>, <em>Y</em><sub>2</sub>, …, <em>Y</em><sub><em>n</em></sub></span>. <span class="math"><em>W</em></span> is a random variable because it depends on the random sample: as we obtain different random samples from the population, the value of <span class="math"><em>W</em></span> can change.</p>
<p>When a particular set of numbers <span class="math">{<em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, …, <em>y</em><sub><em>n</em></sub>}</span> is plugged into <span class="math"><em>h</em></span>, we obtain an <em>estimate</em> of <span class="math"><em>θ</em></span>, denoted <span class="math"><em>w</em> = <em>h</em>(<em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, …, <em>y</em><sub><em>n</em></sub>)</span>.</p>
<p>So we have that:</p>
<ul>
<li><p><span class="math"><em>W</em></span> is a point estimator</p></li>
<li><p><span class="math"><em>w</em></span> is a point estimate</p></li>
</ul>
<p>to evaluate estimation procedures we study various properties of the PDF of <span class="math"><em>W</em></span>. The distribution of an estimator is called its <strong>sampling distribution</strong>. In mathematical statistics, we study the sampling distributions of estimators.</p>
<p><strong>Unbiasedness:</strong> an estimator, <span class="math"><em>W</em></span> of <span class="math"><em>θ</em></span>, is an unbiased estimator if</p>
<p><br /><span class="math"><em>E</em>(<em>W</em>) = <em>θ</em></span><br /></p>
<p>for all possible values of <span class="math"><em>θ</em></span>.</p>
<p>Remarks:</p>
<ul>
<li><p>If an estimator is unbiased, then its PDF has an expected value equal to the parameter it is estimating. However, in any given sample <span class="math"><em>E</em>(<em>W</em>)</span> may not equal <span class="math"><em>θ</em></span>.</p></li>
<li><p>Rather, if we could indefinitely draw samples from the population, getting an estimate each time, and then average these estimates over all random samples we would obtain <span class="math"><em>θ</em></span>.</p></li>
<li><p>This is just a thought experiment, because in reality we have only one sample to work with. But this “what if” property is desirable for estimators.</p></li>
</ul>
<p>If <span class="math"><em>W</em></span> is a <strong>biased estimator</strong> of <span class="math"><em>θ</em></span>, its bias is defined as</p>
<p><br /><span class="math"><em>B</em><em>i</em><em>a</em><em>s</em>(<em>W</em>) = <em>E</em>(<em>W</em>) − <em>θ</em></span><br /></p>
<p>Example: <span class="math">$\bar{Y}$</span> is an unbiased estimator of the population mean, <span class="math"><em>μ</em></span></p>
<p><br /><span class="math">$$\begin{aligned}
 E(\bar{Y}) &amp;= E\left(\frac{1}{n} \sum\limits_{i=1}^{n} Y_{i}    \right) \\ 
            &amp;= \frac{1}{n} E\left(\sum\limits_{i=1}^{n} Y_{i}    \right) 
            = \frac{1}{n} \left( \sum\limits_{i=1}^{n} E(Y_{i}) \right) \\
            &amp;= \frac{1}{n} \sum\limits_{i=1}^{n} \mu                     
            = \frac{1}{n} n \mu                                         
            = \mu                                              \end{aligned}$$</span><br /></p>
<p>Example: <span class="math"><em>s</em><sup>2</sup></span> is an unbiased estimator of <span class="math"><em>σ</em><sup>2</sup></span>.</p>
<p>Let <span class="math">{<em>Y</em><sub>1</sub>, <em>Y</em><sub>1</sub>, …, <em>Y</em><sub><em>n</em></sub>}</span> denote a random sample from the population with</p>
<ul>
<li><p><span class="math"><em>E</em>(<em>Y</em>) = <em>μ</em></span></p></li>
<li><p><span class="math"><em>V</em><em>a</em><em>r</em>(<em>Y</em>)<em>σ</em><sup>2</sup></span></p></li>
</ul>
<p>then</p>
<p><br /><span class="math">$$s^{2} = \frac{1}{n-1}\sum\limits_{i=1}^{n} (Y_{i} - \bar{Y})^{2}$$</span><br /></p>
<p>This is usually called the <strong>sample variance</strong>.</p>
<p>Note: the division by <span class="math"><em>n</em> − 1</span> accounts for the fact that <span class="math"><em>μ</em></span> is estimated by <span class="math">$\bar{Y}$</span> and not known. If <span class="math"><em>μ</em></span> were known <span class="math">$\frac{1}{n} \sum\limits_{i=1}^{n} (Y_{i} - \mu)^{2}$</span>, would be an unbiased estimator.</p>
<p>Unbiasedness has some weaknesses:</p>
<ul>
<li><p>Some very reasonable, even very good, estimates are not unbiased.</p></li>
<li><p>Some unbiased estimates are quite poor.</p></li>
</ul>
<p>For example:</p>
<p><br /><span class="math"><em>W</em> = <em>Y</em><sub>1</sub>(<em>i</em>.<em>e</em>.<em>d</em><em>i</em><em>s</em><em>c</em><em>a</em><em>r</em><em>d</em><em>a</em><em>l</em><em>l</em><em>o</em><em>t</em><em>h</em><em>e</em><em>r</em><em>o</em><em>b</em><em>s</em><em>e</em><em>r</em><em>v</em><em>a</em><em>t</em><em>i</em><em>o</em><em>n</em><em>s</em>)</span><br /></p>
<p>It is an unbiased estimator <span class="math"><em>E</em>(<em>Y</em><sub>1</sub>) = <em>μ</em></span>.</p>
<p>Example: If <span class="math"><em>n</em> = 100</span>, we have one hundred observation of the random variable <span class="math"><em>Y</em></span>, but we discard all but the first to estimate <span class="math"><em>E</em>(<em>Y</em>)</span>.</p>
<p>The weaknesses of unbiasedness show that we need additional criteria to evaluate estimators. Unbiasedness ensures that the sampling distribution of an estimator has a mean value equal to the parameter it is estimating.</p>
<p>We also want to know how spread out it is. The variance of an estimator is called its <strong>sampling variance</strong> because it is the variance associated with the sampling distribution.</p>
<p>Example: <br /><span class="math">$$\begin{aligned}
Var(\bar{Y}) &amp;= Var\left(\frac{1}{n} \sum\limits_{i=1}^{n} Y_{i} \right) \\
             &amp;= \frac{1}{n^{2}} Var\left( \sum\limits_{i=1}^{n} Y_{i} \right) = 
                 \frac{1}{n^{2}} \left( \sum\limits_{i=1}^{n} Var(Y_{i}) \right) \\
             &amp;= \frac{1}{n^{2}} \left( \sum\limits_{i=1}^{n} \sigma^{2} \right) = 
                 \frac{1}{n^{2}} n \sigma^{2} = \frac{1}{n} \sigma^{2} \end{aligned}$$</span><br /></p>
<h1 id="references">References</h1>
<p><a href="http://goo.gl/SIqGID">Wooldridge, Jeffrey (2009) <em>Introductory Econometrics: A Modern Approach 4th Edition.</em></a>.</p>
</body>
</html>
